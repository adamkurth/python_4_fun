## visualize on distribution
df = n-2
x.range = seq(-35,35,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(-t.critical, t.critical), col = "red", lty =2)
abline(v = t.test.stat, col = "blue", lty = 2)
# 2.5 part d
df = n-2
alpha = .05
b1 = summary(r)$coefficients[2, "Estimate"]
se.b1 = summary(r)$coefficients[2,"Std. Error"]
t.test.stat =(b1 - 14)/se.b1 ## subtracting beta value at beta1 = 14
t.test.stat
t.critical = qt(1-alpha, df=df, lower.tail=TRUE)
cat('T-Critical value (rejection region)', t.critical, "with the following df:", df, '\n')
cat('Test Statistic: t.test.stat =', t.test.stat,' > ', t.critical, '= T-critical', '\n')
t.test.stat > t.critical # reject H0
p = pt(q=t.test.stat, df = df)
p
p.val = pt(t.test.stat, df = n-2, lower.tail=FALSE) #one sided test.
cat('P-Value: ', p, '\n')
cat('Significance level (alpha)', alpha, '\n')
p < alpha
cat(' p.val =', p,' > ', alpha, '= alpha' )
#### plotting distribution
df = n-2
x.range = seq(-4,4,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(t.critical), col = "red", lty =2)
abline(v = t.test.stat, col = "blue", lty = 2)
pt(q=abs(t.test.stat), df=df)
pt(abs(t.test.stat), df=df)
1- pt(abs(t.test.stat), df=df)
2*(1- pt(abs(t.test.stat), df=df))
anova(r)
p.val = anova(r)[1,5]
p.val
p.val = anova(r)[]
anova(r)
p.val = anova(r)[1,]
p.val
p.val = anova(r)[1,5]
p.val
anova(r)
anova(r)[1,5]
anova(r)[1,6]
anova(r)[1,4]
anova(r)[1,5]
anova(r)
anova(r)[1,5]
anova(r)
anova(r)[2,5]
anova(r)[2,4]
anova(r)[2,3]
anova(r)[2,2]
anova(r)[2,1]
anova(r)[2,0]
anova(r)p[1,6]
anova(r)[1,6]
anova(r)[1,5]
anova(r)[14]
anova(r)[1,4]
qt(1-.1, df)
1- qt(1-.1, df)
1- abs(qt(1-.1, df))
p.val = anova(r)[1,5]
p.val
2*(1- pt(abs(t.test.stat), df=df))
2*(1-qt(abs(t.test.stat), df=df))
2*(1-qt(abs(t.test.stat), df=df,is.na = F)
2*(1-qt(abs(t.test.stat), df=df,is.na = F))
2*(1-qt(abs(t.test.stat), df=df, is.na = T))
2*(1-qt(abs(t.test.stat), df=df))
2*(1-pt(abs(t.test.stat), df=df))
2*(1-pt(abs(-t.test.stat), df=df))
p = pt(q=31.12326, df)
p
p = pt(q=31.12326, df)*2
p
p = 1-pt(q=31.12326, df)*2
p
p = pt(q=31.12326, df)*2
p
getwd()
setwd("/Users/adamkurth/Documents/RStudio/STP 530/Homework 3/")
data = read.table("CH01PR20 copy.txt")
num.copiers = data$V2
x = data$V2
min.serviced = data$V1
y = data$V1
n = length(y)
colnames(data) <- c("min.serviced", "num.copiers")
plot(min.serviced ~ num.copiers, xlim = c(0,10),
xlab="Number of copiers sercived",
ylab="Minutes spent being serviced",
data = data)
title(main = "Number of copiers sercived vs. minutes spent per call")
r = lm(formula = min.serviced ~ num.copiers,  data=data)
r
abline(coef(r))
##### concise
b1 = summary(r)$coefficients[2, "Estimate"]
se.b1 = summary(r)$coefficients[2,"Std. Error"]
cat('Standard error of b1: ', se.b1, '\n')
t.test.stat = b1/se.b1 # by eq. 1
cat('Test statistic: ', t.test.stat, '\n')
###Computing t-critical
alpha = .10
df = n-2
t.critical = qt(1-alpha/2, df = df)
cat('T-Critical values (rejection region)', -t.critical, t.critical, '\n')
t.test.stat >  t.critical
df = n-2
p.val = anova(r)[1,5]
p.val
p = pt(q=31.12326, df)*2
p.val = 2*(1-pt(q=abs(t.test.stat), df = n-2)) #two tailed test.
cat('P-Value: ', p.val, '\n')
cat('Significance level (alpha)', alpha, '\n')
p.val < alpha
###### manually
x.bar = mean(x); y.bar = mean(y)
ss.xx = sum((x-x.bar)^2) # sum of square deviations
sp.xy = sum((x-x.bar)*(y-y.bar)) #sum of cross products x,y
b1 = sp.xy / ss.xx
b1 # attain correct b1.
b0 = y.bar-b1*x.bar
b0 # attain correct b0.
y.hat = predict(r)
e = y - y.hat
SSE = sum(e^2)
s = sqrt(SSE/n-2) # residuals standard error
se.b1 = s/sqrt(ss.xx)
cat('Standard error of b1: ', se.b1, '\n')
######
## compute t critical.
alpha = .10
df = n-2
t.critical = qt(1-alpha/2, df = df)
cat('T-Critical values (rejection region)', -t.critical, t.critical, '\n')
## visualize on distribution
df = n-2
x.range = seq(-35,35,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(-t.critical, t.critical), col = "red", lty =2)
abline(v = t.test.stat, col = "blue", lty = 2)
# 2.5 part d
df = n-2
alpha = .05
b1 = summary(r)$coefficients[2, "Estimate"]
se.b1 = summary(r)$coefficients[2,"Std. Error"]
t.test.stat =(b1 - 14)/se.b1 ## subtracting beta value at beta1 = 14
t.test.stat
t.critical = qt(1-alpha, df=df, lower.tail=TRUE)
cat('T-Critical value (rejection region)', t.critical, "with the following df:", df, '\n')
cat('Test Statistic: t.test.stat =', t.test.stat,' > ', t.critical, '= T-critical', '\n')
t.test.stat > t.critical # reject H0
p = pt(q=t.test.stat, df = df)
p
p.val = pt(t.test.stat, df = n-2, lower.tail=FALSE) #one sided test.
cat('P-Value: ', p, '\n')
cat('Significance level (alpha)', alpha, '\n')
p < alpha
cat(' p.val =', p,' > ', alpha, '= alpha' )
#### plotting distribution
df = n-2
x.range = seq(-4,4,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(t.critical), col = "red", lty =2)
abline(v = t.test.stat, col = "blue", lty = 2)
p.val = 2*(1-pt(q=abs(t.test.stat), df = df)) #two tailed test.
cat('P-Value: ', p.val, '\n')
p.val = 2*(1-pt(q=abs(t.test.stat), df)) #two tailed test.
cat('P-Value: ', p.val, '\n')
p = pt(q=31.12326, df)*2
p
p.val = anova(r)[1,5]
p.val = anova(r)[1,5]
p.val
anova(r)
anova(r)[1,'Pr(>F)']
p.val = anova(r)[1,5]
p.val = anova(r)[1,5]
df = n-2
p.val = anova(r)[1,5]
cat('P-Value: ', p.val, '\n')
cat('Significance level (alpha)', alpha, '\n')
p.val < alpha
# 2.5 part d
df = n-2
alpha = .05
b1 = summary(r)$coefficients[2, "Estimate"]
se.b1 = summary(r)$coefficients[2,"Std. Error"]
t.test.stat =(b1 - 14)/se.b1 ## subtracting beta value at beta1 = 14
t.test.stat
t.critical = qt(1-alpha, df=df, lower.tail=TRUE)
cat('T-Critical value (rejection region)', t.critical, "with the following df:", df, '\n')
p = pt(q=t.test.stat, df = df)
p
p.val = pt(t.test.stat, df = n-2, lower.tail=FALSE) #one sided test.
p.val
p = pt(q=t.test.stat, df = df) #one sided test
p
cat('P-Value: ', p, '\n')
cat('Significance level (alpha)', alpha, '\n')
p < alpha
cat(' p.val =', p,' > ', alpha, '= alpha' )
df = n-2
x.range = seq(-4,4,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(t.critical), col = "red", lty =2)
abline(v = t.test.stat, col = "blue", lty = 2)
abline(v = p, col = "blue", lty = 2)
p = pt(q=t.test.stat, df = df) #one sided test
cat('P-Value: ', p, '\n')
cat('Significance level (alpha)', alpha, '\n')
p < alpha
cat(' p.val =', p,' > ', alpha, '= alpha' )
#### plotting distribution
df = n-2
x.range = seq(-4,4,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(t.critical), col = "red", lty =2)
abline(v = p, col = "blue", lty = 2)
abline(v = c(alpha), col = "red", lty =2)
abline(v = p, col = "blue", lty = 2)
p = pt(q=t.test.stat, df = df) #one sided test
cat('P-Value: ', p, '\n')
cat('Significance level (alpha)', alpha, '\n')
p < alpha
cat(' p.val =', p,' > ', alpha, '= alpha' )
#### plotting distribution
df = n-2
x.range = seq(-4,4,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(alpha), col = "red", lty =2)
abline(v = p, col = "blue", lty = 2)
p = pt(q=t.test.stat, df = df) #one sided test
cat('P-Value: ', p, '\n')
cat('Significance level (alpha)', alpha, '\n')
p < alpha
cat(' p.val =', p,' > ', alpha, '= alpha' )
#### plotting distribution
df = n-2
x.range = seq(-4,4,length=1000)
t.dist = data.frame(x=x.range, y = dt(x.range,df))
plot(t.dist$x, t.dist$y, type = "l", main= "t-Distribution",xlab = "t", ylab="Density")
abline(v = c(t.critical), col = "red", lty =2)
abline(v = t.test.stat, col = "blue", lty = 2)
anova(r)
tapply
# same idea, but instead
# apply a function to a subset of a vector, grouped by some factor.
tapply(iris$Sepal.Length, iris$Species, mean)
# ON HOMEWORK 2
#bar plot with whiskers
m = tapply(iris$Sepal.Length, iris$Species, mean)
e = tapply(iris$Sepal.Length, iris$Species, sd)
xx = barplot(m, ylim=c(0,8), col=topo.colors(3))
arrows(xx, m+e, xx, m-e, angle=90, code=3, lwd=3, length=.33)
#
str(tapply)
install.packages("reticulate")
library(reticulate)
import matplotlib
os = import('os')
import s
import os
os = import('os')
while (!completed) {
coordinate_menu(image_array, 1000, coordinates, radius1)
intensity <- intensity_peak
avg <- avg_values
spot_estimate_peak <- intensity - avg
print(paste("Peak Estimate for ring 1:", spot_estimate_peak, 'with radius of', radius1))
coordinate_menu(image_array, 1000, coordinates, radius2)
intensity <- intensity_peak
avg <- avg_values
spot_estimate_peak <- intensity - avg
print(paste("Peak Estimate for ring 2:", spot_estimate_peak, 'with radius of', radius2))
coordinate_menu(image_array, 1000, coordinates, radius3)
intensity <- intensity_peak
avg <- avg_values
spot_estimate_peak <- intensity - avg
print(paste("Peak Estimate for ring 3:", spot_estimate_peak, 'with radius of', radius3))
completed <- TRUE
}
source("~/Library/CloudStorage/GoogleDrive-amkurth@asu.edu/My Drive/vscode/project.dataset.R", echo=TRUE)
install_miniconda()
install.packages("h5")
install.packages("h5")
update
update()
install.packages('h5')
create_scatter <- function(x, y, z, highlight_x=NULL, highlight_y=NULL) {
coordinates_and_intensities <<- cbind(c(x), c(y), c(z))
coordinates <<- coordinates_and_intensities[, 1:2]
print(coordinates_and_intensities)
z <- coordinates_and_intensities[, 3]
library("scatterplot3d")
scatterplot3d(coordinates[, 1], coordinates[, 2], z, color=z, col.axis="viridis", pch=19)
if (!is.null(highlight_x) && !is.null(highlight_y)) {
highlight_z <- coordinates_and_intensities[highlight_x, highlight_y]
print(paste("Intensity value", highlight_z, "\n"))
points3d(highlight_x, highlight_y, highlight_z, col="red", pch=3, cex=2, add=TRUE)
}
colorbar3d(col.axis="viridis", title="Intensity")
xlab("X Coordinate")
ylab("Y Coordinate")
zlab("Intensity")
title("3D Scatter Plot of (X, Y, Intensity)")
}
source("~/Library/CloudStorage/GoogleDrive-amkurth@asu.edu/My Drive/vscode/project.dataset.R", echo=TRUE)
install.packages('rhdf5')
version
library(ggplot2)
wd <- getwd()
cat("\nWorking directory: ", wd)
load_file_h5 <- function() {
filename <- "DATASET1_8_16_23-1.h5"
if (!file.exists(filename)) {
cat("File not found within working directory.")
return()
}
tryCatch({
f <- h5file(filename, "r")
cat("\nLoaded file successfully.", filename)
}, error = function(e) {
cat("\nAn error has occurred:", e$message)
})
}
PeakThresholdProcessor <- R6Class("PeakThresholdProcessor",
public = list(
array = NULL,
threshold_value = 0,
initialize = function(array, threshold_value = 0) {
self$array <- array
self$threshold_value <- threshold_value
},
set_threshold_value = function(new_threshold_value) {
self$threshold_value <- new_threshold_value
},
get_coordinates_above_threshold = function() {
coordinates <- which(self$array > self$threshold_value, arr.ind = TRUE)
return(coordinates)
}
)
)
rm(list=ls()) # It's a good practice to clear working space before starting a new data
install.packages("faraway")
install.packages("car")
install.pacakges("Hmisc")
install.packages("Hmisc")
install.packages("psych")
library(faraway)
library(car)
library(Hmisc)
# Because the describe() function in Hmisc and psych packages share the same name,
# loading the psych package after loading the Hmisc package will overwrite the
# describe() function provided by Hmisc. To make both functions available to you,
# you can rename them like below.
H.describe <- describe
H.describe
library(psych)
P.describe <- describe
# Load the data provided by a package by data()
data(airquality)
# Take a quick look at the dataset. Note there are NA's in the dataset.
head(airquality)
# Use the following code to view the manual of the data provided by the package
?airquality
# Learn more about the structure of the data
str(airquality)
summary(airquality)
H.describe(airquality)
P.describe(airquality)
hist(airquality$Ozone)
hist(airquality$Solar.R)
hist(airquality$Wind)
hist(airquality$Temp)
pairs(airquality[ , 1:4])
mod <- lm(Ozone ~ Solar.R + Wind + Temp, data=airquality, na.action=na.exclude)
summary(mod)
plot(fitted(mod), residuals(mod))
plot(airquality$Solar.R, residuals(mod))
plot(airquality$Wind, residuals(mod))
plot(airquality$Temp, residuals(mod))
# Studentized residual plot
plot(fitted(mod), rstudent(mod))
# The following code helps you find out which point is the outlier
plot(fitted(mod), rstudent(mod), type="n")
text(fitted(mod), rstudent(mod), names(rstudent(mod)))
plot(fitted(mod), residuals(mod))
hist(residuals(mod))
qqnorm(residuals(mod))
qqline(residuals(mod))
airquality$log.Ozone <- log(airquality$Ozone)
logmod <- lm(log.Ozone ~ Solar.R + Wind + Temp, data=airquality, na.action=na.exclude)
summary(logmod)
plot(fitted(logmod), rstudent(logmod))
plot(airquality$Solar.R, residuals(logmod))
plot(airquality$Wind, residuals(logmod))
plot(airquality$Temp, residuals(logmod))
qqnorm(residuals(logmod))
qqline(residuals(logmod))
plot(fitted(logmod), rstudent(logmod), type='n')
text(fitted(logmod), rstudent(logmod), names(rstudent(logmod)))
rstudent(logmod)[21] # double check
airquality.reduced <- airquality[-21, ]
logmod.reduced <- lm(log(Ozone) ~ Solar.R + Wind + Temp, data=airquality.reduced, na.action=na.exclude)
plot(fitted(logmod.reduced), rstudent(logmod.reduced))
qqnorm(residuals(logmod.reduced))
qqline(residuals(logmod.reduced))
summary(logmod.reduced)
lambda <- powerTransform(mod)$lambda
lambda
airquality$bc.Ozone <- airquality$Ozone ^ lambda
bc.mod <- lm(bc.Ozone ~ Solar.R + Wind + Temp, data=airquality, na.action=na.exclude)
# Residual plot of the box-cox-transformed model
plot(fitted(bc.mod), rstudent(bc.mod))
# Q-Q plot of the box-cox-transformed model
qqnorm(residuals(bc.mod))
qqline(residuals(bc.mod))
# Shapiro test
ncvTest(bc.mod)
ncvTest(logmod)
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
# Impression: The Box-cox indeed removed most of the heteroskedasticity. It's
# interesting that the outlier is not longer showing. Comparing the ncvTest
# results between the Box-Cox transformed model and the log-transformed model,
# the Box-Cox transformed model gives a larger p-value, which means the residual
# variances are more constant than the log-transformed model.
# Which transformation do you prefer to use?
setwd("/Users/adamkurth/Documents/vscode/Python/Neural Networks from scratch/digit-recognizer/")
data = read.csv('output.csv')
rm(list = ls())
setwd("/Users/adamkurth/Documents/vscode/Python/Neural Networks from scratch/digit-recognizer/")
data = read.csv('output.csv')
View(data)
source("~/.active-rstudio-document", echo=TRUE)
print(data[1,:])
print(data[1,])
source("~/.active-rstudio-document", echo=TRUE)
